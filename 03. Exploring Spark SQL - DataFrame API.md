# Exploring Spark SQL

## Introduction

> Spark SQL is one of the module in the Spark eco-system to query and analyze structured and semi-structured data.

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdjRxpcGySZ0tYDoTqHW9a3bV8Eld5lkXF4a7w7V4UPPkTbtE_YIBXdvg-NUjYzCFMjIkWQCdpDM7K_qJ9VgfYAzN5vGb2WdAIXRXW8zKVjCQKV5CKeihUsaZAtLiWGvpvSqg2Y5dLQUrzRIMuG9Ib0mQo?key=yGW25KMloT80Lch6YWjT9A)

In any spark application we perform three steps

1. Load the data
2. Process the data
3. Write the result

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcQ_-LDw6t_LRynPbGhvISfdtXrd4lU0llsH3ET5CxNSteHCRpbEG9QokYigCuspi4v1d3jAuFtmgeVDGS4cPCnbkX9eb_2sSkyfaTsEtWz73VykDcX4LjWGtjxQzU7--KZJMS9CfpbevexyfnYp4r0VpKN?key=yGW25KMloT80Lch6YWjT9A)

## DataFrame

> A DataFrame is a distributed collection of data organized into named columns

* It is similar to a table in RDBMS; conceptually you can think of DF as a in-memory table.
* Provides different operations to filter, grouping, soring, aggregations
* DF can be created from different sources (HDFS, Cloud Storage, RDBMS, NoSQL, CSV, JSON, Parquet)

**Important Points**

1. The primary interface to create DataFrame is via DataFrameReader (C)

   ```
   dfr = spark.read
   ```

2. All the methods to create DataFrame is present inside DataFrameReader

   - .csv -> DataFrame
   - .json -> DataFrame
   - .jdbc -> DataFrame
   - .parquet -> DataFrame



















