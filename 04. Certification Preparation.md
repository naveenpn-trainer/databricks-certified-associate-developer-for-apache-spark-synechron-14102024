

![image-20241017105812471](C:\MyTrainings\databricks-certified-associate-developer-for-apache-spark-synechron-14102024\imgs\04. Certification Preparation\image-20241017105812471.png)

**PySpark**

```
sample_data = [
("ProductA",10),
("ProductB",30),
("ProductC",40),
("ProductA",10),
("ProductA",10),
("ProductB",70),
("ProductA",90),
("ProductC",100),
]

rdd = spark.sparkContext.parallelize(sample_data,3)

# Narrow Transformation - .map(func)
val sales_rdd = rdd.map(e=>lambda e : (e[0],e[1]))

# Wide Transformation
total_sales = sales_rdd.reduceByKey(lambda x,y:x+y)

(ProductA,120)
(ProductB,100)
(ProductC,140)

# Narrow Transformation
top_sales = total_sales.filter(lambda x:x[1]>100)

# Action 
top_sales.count()



```

**Apache Spark**

```
val sample_data = List(
("ProductA",10),
("ProductB",30),
("ProductC",40),
("ProductA",10),
("ProductA",10),
("ProductB",70),
("ProductA",90),
("ProductC",100),
)
val rdd = spark.sparkContext.parallelize(sample_data,3)

# Narrow Transformation - .map(func)
val sales_rdd = rdd.map(e=>(e._1,e._2))

# Wide Transformation
val total_sales = sales_rdd.reduceByKey(_+_)

(ProductA,120)
(ProductB,100)
(ProductC,140)

# Narrow Transformation
val top_sales = total_sales.filter(x=>x._2 > 100)

# Action 
top_sales.count()




rdd = sc.parallelize(list(range(1,11)))
rdd.reduce(lambda x,y:x+y)
reduce_rdd.collect()
```

