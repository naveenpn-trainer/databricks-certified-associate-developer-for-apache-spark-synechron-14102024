# Getting Started with Apache Spark and Core API's (RDD's)

> Apache Spark is an **in-memory cluster computing framework** designed to handle a wide range of big data workloads.

1. Data Integration and ETL
2. High Performance Batch Computation
3. Machine Learning Analytics
4. Real-time stream processing
   - Apache Storm
   - Spark (Spark Streaming (RDD's)  and Structured Streaming (Spark SQL))
   - Kafka Streams (Important)
   - Apache Flink (5G for Data Processing)


## What is PySpark

> PySpark is the Python API for Apache Spark

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXe0AdcvUHGLLNvNdrfJtqZxHbbJyN7a3fpGgvVeiImgqRhnAkTn5Q5sOtCI77FD78jcO-RvWsR9CxvjUX3TmkShDqluSj9hOWBBBuzOi9o8PPW0fclQWZxNU_Ti73SIfBuYCAnJWuzhf12KBkVs3wYbJmo?key=uvmlVet7-pBAx-jz0PuzLA)

## How Spark is Faster

There are two reasons what makes spark faster

1. Advance DAG execution
2. Data sharing

```
JOIN
FILTER

FILTER
JOIN
```

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXe-AtA2sQaBSB2L4BvwsqggloH0oWpVLHaVxZA1H1AqmjD-Se65N4x_MSbqe8Yowny2z_s0DX73C8pAtyT83OFmcX6x8sjKMckcIPdmkN4aVzTvV8RAl0lWiUs7sR0jB1BzoDuzBAWvhJVZoFpnii3UMPsW?key=uvmlVet7-pBAx-jz0PuzLA)

## Spark Ecosystem

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeEH_Vpd3yO2S3vApdBfSw0vXBhyQKjOmUlGnscODPLwLdsws6fSiT30lsVfNVyDHLP7wHz4ZlTiMrE3mhN8Pr927KEaDmip5jx89PX4VqWrwUTGWfIWJt0DpfUjDIwZkz8V_gJ_Gt76EDY-0d6gR5GJ4w?key=uvmlVet7-pBAx-jz0PuzLA)

## Spark Interactive Shell

> Refers to the interactive command line interface by Apache Spark using which we can type and execute spark programs quickly

Spark provides two interactive shells

1. Spark Shell (Scala)

   ```
   $> spark-shell
   ```

   

2. PySpark Shell (Python)

   ```
   $> pyspark
   ```

   

## Different API's to interact with Spark

1. Low Level - RDD's (Deprecated)
2. High Level API - Spark SQL (DataFrame, Dataset (not supported with Python))

## Exploring Spark Core API's (RDD's)

> Resilient Distributed Dataset are the building blocks of any spark application
>
> RDD is the fundamental data structure of apache spark 

## Partitions

> RDD is a collection of objects that is partitioned and distributed across nodes in a cluster
>
> A partition is a logical chunk of a large dataset

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcW3cecP5h3fCvbKaF2fXs4ePIsS49m1CIfFonKcZwKgLeU5-cDaF8DviY7KxiHQtNfWa6qu0q4M8AaW71W91fu9dQ2Kr2VP_86Hhqrw-nK8BZFmBF9Gsprvbgg5eBjWrgdN5VhU75NKJGnIk2DmNgecxbk?key=uvmlVet7-pBAx-jz0PuzLA)



## RDD Creation

There are two popular ways to create an RDD

1. Create an RDD from Collection
2. Create an RDD from external source

**Important Points**

* All the methods to create RDD is present inside SparkContext (sc)

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXc8JL-ZSV_OmuBsOwZe6TsBYdIxFz7O4RODlubNUJG561AUB1lcQR0bMH2K-peXIaEQ7PmH4GoeJmzyDVMCVf2A2CTkEwCvOgJRXfj42NYwGDAz5bUlUEzWWl7v7keRx865ZQx-h9jYXJ9juQg8arqtLY0?key=uvmlVet7-pBAx-jz0PuzLA)

## RDD Operations

Once an RDD is created we can perform two types of operations

1. Transformation
2. Actions

### Transformation

* Transformation creates a new RDD / DataFrame from an existing RDD
* E.g : .map(), .filter()

### Actions

* Actions are the operation on the RDD / DataFrame to carry out final computations
* Act on the entire RDD and produce result
* E.g .collect()





